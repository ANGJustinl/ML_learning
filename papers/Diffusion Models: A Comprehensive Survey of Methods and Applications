Diffusion Models: A Comprehensive Survey of Methods and Applications》（2022）


# 扩散模型的基本原理与三种主要形式

扩散模型是一类生成式模型，其基本思想是通过**正向扩散过程**逐步向数据注入噪声，最终使数据变为简单先验分布，再通过学习一个**逆向去噪过程**从噪声还原数据。当前研究主要有三种等价或相关的数学形式：

* **DDPM (去噪扩散概率模型)**：采用两个马尔可夫链，一个正向链将数据逐步加噪（通常采用逐步添加高斯噪声），直到接近简单分布；一个逆向链通过神经网络参数化的转移核逐步去噪生成样本。在生成时，先从先验（如标准高斯）采样噪声，然后沿逆向马尔可夫链逐步去噪得到新数据。

* **SGM (基于分数的生成模型)**：核心在于估计数据分布的**分数函数**（即对数密度的梯度）。SGM首先对数据施加不同强度的高斯噪声，并训练一个**噪声条件分数网络**（Noise-Conditional Score Network，NCSN）来估计各噪声水平下的分数函数。生成时，利用这些分数函数通过Langevin采样或SDE等算法逐步去噪。例如，退火Langevin动力学（Annealed Langevin Dynamics）即利用逐渐降低的噪声水平迭代采样生成数据。值得注意的是，DDPM和SGM在训练目标上密切相关：某些权重函数下，DDPM的训练损失与多尺度去噪分数匹配的目标形式相同。

* **Score SDE (分数随机微分方程)**：将上述过程推广到连续时间。在Score SDE框架中，正向扩散由一个Itô SDE驱动，逆向过程则由相应的逆时向SDE或概率流ODE描述。具体地，对应DDPM的离散步骤有其连续极限形式，对应的逆SDE方程保证与正向过程相同的边缘分布，可通过数值SDE/ODE求解器（如E-M方法、Heun法等）来采样。Score SDE统一了DDPM和SGM：两者的正向过程都可视为特定SDE的离散化，学习分数函数即可解锁逆向SDE/ODE，从而生成样本。

上述三种形式本质上都是对同一扩散-逆扩散流程的不同表述：DDPM强调离散马尔可夫链、SGM强调显式学习分数函数，Score SDE则给出连续极限和数值求解方案。这些方法的训练目标在适当配置下等价，均旨在最大化数据似然或其下界，从而获得高质量生成效果。

## 提高采样效率的方法

扩散模型生成样本通常需要较多步迭代，采样成本高。近期研究在**无学习采样（solver-based）**和**基于学习的采样**两方面提出优化策略：

* **无学习采样（求解器方法）**：通过改进数值求解器减少离散步数。一类方法直接离散化逆向SDE（式(18)）来采样。例如，原始DDPM采样即对应反向SDE的简单欧拉离散；退火Langevin动力学（ALD）等方法采用MCMC逐步退噪，具有正确的边缘分布；一致退火采样（CAS）和临界阻尼Langevin扩散（CLD）则通过添加额外噪声或动量变量改进采样速度与质量。另一类方法直接解概率流ODE（式(19)）。著名的DDIM方法等价于特殊的确定性解算方案，将DDPM拓展到非马尔可夫链，允许利用较少步数生成样本。多项研究（如Heun方法、高阶ODE求解器、Diffusion Exponential Integrator等）进一步使用更高阶或定制求解器显著减少采样步数，同时保持较好质量。

* **基于学习的采样**：通过额外训练或优化策略改进采样流程。**优化离散方案**方法（Watson等）通过动态规划或微分搜索选择最优时间步，以最大化采样质量指标（如KID）；Dockhorn等基于泰勒近似训练二阶求解器，提高了生成质量。**截断扩散**方法提前终止正向扩散，然后从非高斯分布（例如由预训练GAN/VAE生成的样本）开始逆扩散，从而减少步数。**知识蒸馏**策略（如Progressive Distillation）则将完整扩散过程压缩为更短的深度神经网络采样器，通过训练使其输出近似原始扩散采样结果，可将步数减半以上。这些学习型方法通常以牺牲少许样本质量为代价显著加快了采样速度。

## 提高模型似然估计的方法

标准扩散模型训练使用变分下界（VLB），但该下界可能不够紧致，影响模型对数似然。近年来工作集中在**噪声调度优化**、**反向方差学习**和**精确似然计算**等方向：

* **噪声调度优化**：传统扩散过程中噪声调度（如β系数序列）通常手工设定。iDDPM提出使用余弦函数调度来替代线性调度，可提高模型对数似然。VDM等方法直接将噪声调度参数化为可训练网络，通过联合训练噪声调度和模型参数来最大化VLB。研究发现，VLB可以简化为只依赖信噪比（SNR）的形式，因此噪声调度对VLB影响有限，但优化调度可减小蒙特卡洛估计方差，间接提高训练效果。

* **反向方差学习**：经典DDPM假设逆向扩散核的方差固定。iDDPM等工作通过参数化逆向方差（如对数域的线性插值）并一并学习，显著提升了对数似然和采样速度。Analytic-DPM通过利用预训练分数函数计算最优方差，进一步紧化了VLB，实现更高的似然值。

* **精确似然计算**：基于Score SDE的框架，可将采样视为求解神经ODE，从而理论上可精确计算对数似然。相关工作（ScoreFlow系列）提出利用ODE积分与随机迹估计（Skilling-Hutchinson方法）计算精确似然，并设计特定权重函数使优化目标直接对应数据的对数似然上界。Lu等还将高阶分数匹配纳入目标，对原始ScoreFlow进行了改进。这些方法虽然计算成本高，但为扩散模型的似然评估提供了理论和实践方案。

## 适应特殊结构数据的扩散建模

许多数据具有离散性或对称不变性，而标准扩散模型只适用于连续欧氏空间，需要改进以适应特殊结构：

* **离散数据**：高维离散数据无法直接加高斯噪声。相关工作采用不同的扩散核设计：VQ-Diffusion使用离散空间上的随机游走或掩码作为正向过程；D3PM引入吸收态或离散化高斯核进行加噪；Campbell等利用连续时间马尔可夫链框架建立离散扩散模型，并证明其采样性能优于离散版本。Concrete Score Matching（CSM）将离散概率分布的变化定义为“离散分数”，即概率随输入有限差分的变化率，使得可训练此分数函数并用于MCMC采样。另有方法基于Doob’s h-变换，通过在逆扩散中添加额外力项将样本约束到特定区域，以处理受约束的离散和结构化域。

* **不变结构数据**：如图数据的置换不变性、分子构型的平移旋转不变性等。Niu等提出在图扩散模型中使用置换等变的图神经网络（EDP-GNN）作为分数网络；GDSS进一步采用连续时间图SDE，使用消息传递确保逆过程的置换不变性。在分子构型生成方面，Shi等、Xu等引入了在初始先验和转移核上满足平移旋转不变性的扩散马尔可夫链，证明此类链对齐保持了生成分布的不变性，可用于生成不变的分子构型。

* **流形数据**：许多数据理论上位于低维流形上。针对已知流形，Riemannian SGM（RSGM）将分数扩散推广到紧致黎曼流形（如球面、环面），使用测地随机游走近似采样，并给出流形上逆向扩散公式；Riemannian DPM（RDM）以变分下界为目标，从外部欧氏空间的视角处理流形数据。对于未知流形，许多方法首先通过自编码器将数据映射到低维潜空间再进行扩散。Latent SGM（LSGM）联合训练VAE和分数模型，将VAE的ELBO与扩散的分数匹配目标合并，在潜空间生成样本，提升了采样效率并允许处理离散数据。Latent Diffusion Model（LDM）和DALL·E2等方法则先训练自编码器，再在潜空间训练扩散模型，实现了高分辨率图像生成的效率提升。

## 与其他生成模型的关系与融合

扩散模型与其他生成模型存在内在联系，并可相互融合提升性能：

* **VAE**：DDPM可视为一个具有固定编码器的层次化VAE，其中正向扩散是线性高斯编码过程，逆向扩散为解码器（每步共享参数）。连续时间下，研究表明分数匹配目标等价于深层VAE的ELBO，暗示Score SDE可被看作“无限层次VAE”的极限。Latent SGM进一步指出，潜空间ELBO可视为分数匹配目标的特殊形式（分数模型可视为无限深VAE），从而将扩散生成与VAE框架结合。

* **GAN**：GAN生成器/判别器的对抗训练与扩散模型互补。一方面，可在GAN训练中为判别器输入注入扩散噪声（噪声调度由扩散模型确定），扩展生成器和判别器的分布支持，提升训练稳定性（如Wang等的Diffusion-GAN）。另一方面，可用GAN替代扩散每一步的去噪分布假设，例如Xiao等提出用条件GAN对每步去噪建模，从而允许更大步长采样并加速生成。

* **Normalizing Flow**：Flow模型要求可逆映射，而扩散模型不需全局可逆。一些工作将两者结合：DiffFlow将Flow和扩散优点结合，能够生成更清晰边界且使用更少扩散步数；Implicit Nonlinear Diffusion Model（INDM）先用Flow将数据映射到潜空间，再在潜空间内进行扩散，从而学习一个由Flow决定漂移项和扩散项的非线性SDE，改善了似然和采样速度。

* **自回归模型 (ARM)**：ARM通过逐维条件生成，效率较低。为此，Autoregressive Diffusion Model（ARDM）提出了一种无序的自回归扩散框架，使得模型在训练时与普通扩散类似，在测试时可并行生成任意顺序数据。Meng等还结合随机平滑，将原始分布与平滑核卷积，先用自回归模型学习平滑分布，再进行梯度去噪生成，提高了采样质量。另外的AR-CSM方法对自回归模型的条件分布使用分数匹配，无需归一化常数，并用Langevin采样从中生成样本。

* **能量模型 (EBM)**：EBM可视为约束的分数模型。Salimans等对比发现，在相当的模型结构下，EBM（受限分数模型）与无约束分数模型效果相当。另一方面，Gao等提出在扩散逆过程中学习一系列EBM（通过“恢复似然”最大化每个噪声条件下的样本概率），使得长程MCMC样本仍保持真实感，从而实现高质量生成。这等价于将扩散过程中的每步视作条件EBM学习问题，缓解了传统EBM难训练的问题。

## 各领域的应用

扩散模型凭借灵活性和强大学习能力，在多个领域取得显著应用成果，包括但不限于：

* **计算机视觉**：扩散模型广泛应用于图像生成与编辑。无条件和条件扩散可用于图像合成；超分辨率、图像修复、去噪、风格迁移、图像编辑等任务均可通过扩散逆过程实现。例如，DDPM可用于图像修复和超分辨率（Rombach等Latent Diffusion、Saharia等的Imagen等），在FID/PSNR指标上超越传统方法。扩散也用于**语义分割**（生成语义掩码）、**视频生成**（通过时间条件扩散保持帧间一致性）、**点云补全与生成**（Luo等模型点云漫游）及**异常检测**（使用扩散重构判别异常）。总之，扩散模型在视觉领域可处理合成、恢复、编辑与分析等多种任务。

* **自然语言处理**：扩散思想被用于文本和语音生成。近年来已有基于扩散的文本生成模型（Treating discrete tokens via离散扩散链或变换器架构）以及基于SDE的语音合成模型（如DiffWave、Grad-TTS等）。扩散方法可以在保证生成多样性的同时，改进多模态结合的语言生成任务。

* **跨模态生成**：文本引导的图像生成是当前热点。由Stable Diffusion、DALL·E 2、Imagen等代表的扩散模型，能够将文本描述转化为高质量图像。此外，场景图到图像、文本到3D模型、文本到动作序列、文本到视频、文本到音频等多种跨模场景也采用扩散模型。例如，Scene Graph to Image生成方法通过条件扩散将图结构信息映射为图像；Text-to-3D、Text-to-Motion、Text-to-Video、Text-to-Audio等模型通过多模条件扩散生成相应内容。

* **时序数据建模**：扩散模型在时间序列和信号处理中表现出潜力。在**时序插补**中，CSDI等方法利用扩散模型推断缺失值， outperform传统方法。在**时序预测**中，TimeGrad等将扩散嵌入自回归网络，通过估计梯度生成未来时间点分布。在**波形信号处理**（如语音）领域，WaveGrad等模型使用扩散逆过程将噪声转为语音波形，实现高质量合成。

* **稳健学习**：扩散模型可用作对抗防御的净化器。DiffPure等方法对抗攻击样本先加少量噪声再逆扩散，实现数据净化；ADP使用训练好的EBM（通过分数匹配）快速纯化噪声图像，并引入随机化方案；PGD (Blau等)将扩散作为预处理提炼输入以增强模型鲁棒性。总体而言，将扩散模型视为图像“纯化”管道，可在不改变分类器的前提下有效抵御扰动。

* **交叉学科应用**：在科学领域，扩散模型已用于药物设计、材料设计和医学成像等问题。**药物/分子设计**中，利用扩散生成分子图和原子构型，例如GeoDiff对分子扭转角进行扩散、Torsional Diffusion对三维结构进行扩散、TargetDiff在给定蛋白质引导下生成配体等。**材料设计**方面，CDVAE对晶体结构空间进行扩散并纳入对称性不变性；Luo等对抗体结构引入等变扩散生成抗体。**医学成像重建**方面，扩散模型用于CT/MRI的逆问题：如Song等将分数模型与观测相结合进行重建；Peng等通过引导逆扩散使重建过程符合观测k空间数据。这些应用展示了扩散模型在科学计算和工程领域的广泛潜力。

## 未来研究方向与展望

作者指出了扩散模型研究的若干前沿方向：

* **重审模型假设**：传统扩散假设前向过程在有限时间内可完全抹去信息，但实际中信息不可完全消失。探索何时终止扩散（平衡采样效率与质量）以及借鉴Schrödinger桥和最优传输等新技术，将是重要方向。

* **理论解析**：目前对扩散模型为何优于其他生成模型（如GAN、VAE、EBM、ARM）的理解有限。需要深入研究其本质区别、决定生成质量与似然的因素，以及各种超参（步数、噪声调度等）的系统选择方法，以指导实践。

* **隐空间表征**：扩散模型不像VAE/GAN，通常缺乏低维有效隐表示。研究如何让扩散模型提供有意义的潜在空间（例如结合Autoencoder、变分指标等）将对提高采样效率和语义可操控性非常关键。

* **AIGC与扩散基础模型**：生成式预训练已在大型语言/视觉模型（GPT、ChatGPT等）取得突破。将生成预训练思路（decoder-only）迁移到扩散模型、训练大规模扩散基础模型，并探索其“涌现能力”，是新兴研究热点。尤其，将大型语言模型与扩散模型结合（如文本指导图像编辑）被认为前景广阔。

上述方向指明了扩散模型未来研究的重点领域：结合理论与实践、探索更强大的模型结构和训练机制，以进一步提升扩散模型的性能和应用广度。

**参考文献：** 文中引用均来自等。
